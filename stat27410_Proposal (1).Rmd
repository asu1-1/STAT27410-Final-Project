---
title: "STAT 27410 Final Project Proposal - A Bayesian Approach to Portfolio Management"
author: "Andrew Su, Scarlett He"
fontsize: 12pt
bibliography: references.bib
output:
  pdf_document: default
  html_document:
    df_print: paged
  word_document: default
  geometry: margin=0.75in
fig_crop: no
---

```{r echo=FALSE, message=FALSE}
library(knitr)
knitr::opts_chunk$set(fig.width=4, fig.height=3, message=F, warning=F)
options(scipen=6, digits=6)
```

# 1. Introduction

Portfolio management is a complex endeavor that involves not only selecting appropriate investments but also navigating the uncertainties posed by market dynamics and economic conditions. While diversification across sectors and asset classes is widely regarded as a cornerstone of effective portfolio construction, managing risk and optimizing returns remains a significant challenge. This study seeks to explore innovative approaches to enhance investment decision-making by comparing traditional frequentist methods with Bayesian techniques in the context of portfolio optimization.

Over a three-year period from January 1st, 2022, to January 1st, 2025, we analyze historical stock prices for five individual stocks:

* Apple Inc. Common Stock (AAPL) [@AAPL]
* Coca-Cola Company (The) Common Stock (KO) [@KO]
* Costco Wholesale Corporation Common Stock (COST) [@COST]
* Advanced Micro Devices Inc. Common Stock (AMD) [@AMD]
* Salesforce Inc. Common Stock (CRM) [@CRM]

Two sector specific ETFS:

* Invesco QQQ Trust (QQQ) [@QQQ]
* the Consumer Discretionary SPDR Select Sector Fund (XLY) [@XLY]

As well as S&P 500 index (SPX) [@SPX] as a baseline for comparison. 

All data used is from the 3-year timespan of January 1st, 2022 to January 1st, 2025 (753 trading days) and obtained directly from the Nasdaq stock exchange. Data was sourced directly from the Nasdaq stock exchange, ensuring a robust empirical basis for our analysis.

The selection of these securities reflects a strategic focus on diverse industries and market segments. We chose individual companies and ETFs to represent both the Technology and Consumer sectors, ensuring a mix of defensive and cyclical stocks while avoiding undue similarity among the selected investments. This approach aims to capture variations in market performance and provide a comprehensive view of portfolio dynamics.

For each of the securities, separate datasets were used. For each security, the data set include the following variables:

* **Date**: The date of the trading day
* **Close/Last**: The price of the security at the end of the trading day
* **Volume**: The total number of shares traded during the trading day
* **Open**: The price of the security at the start of the trading day
* **High**: The highest price of the security during the entire trading day
* **Low**: The lowest price of the security during the entire trading day

For the purpose of our analysis, we will only be looking at the Date, Close/Last, Open and Volume categories across each of the securities to build a fundamental AR model to capture essential market dynamics and trends. 

To determine weights of allocation, we will run regression models to predict the future prices of each stock, calculating the percentage changes of each and assigning portfolio weights through convex optimization. For instance, for a certain stock A, our model will include the previous prices of stock A into our calculation for future price of A, while also incorporating the sector-ETF and SPX as measures of how well the economy is doing. 

We define the stock price of stock \( i \) at time \( t \) as \( P_{i,t} \). The predictive model for stock prices is given as follows:

For each technology sector stock, the model is expressed as:

$$
\begin{aligned}
P_{i,t+1} &= \beta_1 P_{i,t} + \beta_2 P_{i,t-1} + \\
&\quad \beta_3 \text{QQQ}_{t} + \beta_4 \text{SPY}_{t} + \sum_{j \neq i} \text{COV}(P_{i,t}, P_{j,t})
\end{aligned}
$$

For each consumer sector stock, the model is expressed as:

$$
\begin{aligned}
P_{i,t+1} &= \beta_1 P_{i,t} + \beta_2 P_{i,t-1} + \\
&\quad \beta_3 \text{XLY}_{t} + \beta_4 \text{SPX}_{t} + \sum_{j \neq i} \beta_{5i} \text{COV}(P_{i,t}, P_{j,t})
\end{aligned}
$$

The percentage change in stock price from time \( t \) to \( t+1 \), denoted as \( R_{i,t} \), is given by:

$$
R_{i,t} = \frac{P_{i,t+1} - P_{i,t}}{P_{i,t}}
$$

As opposed to a pure Auto-Regressive (AR) model, we will consider exogenous variables that aim to capture aspects of the economy that cannot be fully captured with our limited selection of stocks. This would allow for specific analysis while also factoring in more complex market trends. \textit{Note: Our models are subject to change}.

With computed stock price percentage changes, we will use convex optimization to determine the portfolio weights, where volume traded will play a role in managing the risk assigned to each stock. 

We will then compare these frequentist approaches with Bayesian methods, where we will assume a prior distribution (multivariate normal or student-t) on an asset, and then continuously update the distribution using the market data as the likelihood function to generate a posterior distribution that gives the most likely expected return for assets and measuring uncertainty through credible intervals. These returns can then be used in combination with the uncertainty measures to allocate portfolio weights.

This study not only sets the stage for a detailed comparison between frequentist and Bayesian methods in portfolio optimization but also provides insights into the important features to capture comprehensive market dynamics.

\newpage

# 2. Exploratory Data Analysis

We will start by looking at the trends and changes in each of the securities over this 3 year period. 

```{r, echo=FALSE, out.width = "90%", fig.align='center'}
knitr::include_graphics("/Users/scarlett_hjq/Desktop/STAT27410/STAT27410-Final-Project/Closings.png")
```

These graphs show the trends of the 7 securities over the 3-year period. It can be noted that some graphs have similar trends. Thus, we look at correlation matrix of those stock, and we found the correlation is relatively high due to the choice of our stocks are all well known and having a high market share.
```{r, echo=FALSE, out.width = "80%", fig.align='center'}
knitr::include_graphics("/Users/scarlett_hjq/Desktop/STAT27410/STAT27410-Final-Project/all_stocks_correlation.png")
```

```{r, echo=FALSE, out.width="50%"}
knitr::include_graphics("/Users/scarlett_hjq/Desktop/STAT27410/STAT27410-Final-Project/consumer_stocks_correlation.png")
knitr::include_graphics("/Users/scarlett_hjq/Desktop/STAT27410/STAT27410-Final-Project/technology_stocks_correlation.png")
```

\newpage

# 3. Frequentist Analysis

## 3.1 Proposed Frequentist Model(s)

### 3.1.1 Time Series Models

Time series analysis emerges as a fundamental approach for financial data forecasting due to financial data's natrual of time dependency. It offers ways to locate patterns from historical fianncial data and form predictions to provide insights on investment decisions.

The strength of time series analysis lies in its capacity to clean data and remove confounding variables and white noises. It also handles the non-independency between data in a sequence of time.

(ADD WHY LOG RETURN)
We are trying to predict the log return of a stock price.
The percentage change in stock price from time \( t \) to \( t+1 \), denoted as \( R_{i,t} \), is given by:

$$
R_{i,t} = \frac{P_{i,t+1} - P_{i,t}}{P_{i,t}}
$$
We are predicing $\ln R_{i,t}$.
We use 2/3 of our data as training data.

```{r, echo=FALSE, out.width = "80%", fig.align='center'}
knitr::include_graphics("/Users/scarlett_hjq/Desktop/STAT27410/STAT27410-Final-Project/log_return_plot.png")
```
We see the graph of log return shows like white noise around mean 0.

In our methodological approach, the initial phase involved testing for stationarity in the financial time series data. The log transformation of the return is stationary according to the Augmented Dickey-Fuller (ADF) test.

```{r, echo=FALSE, out.width = "60%", fig.align='center'}
knitr::include_graphics("/Users/scarlett_hjq/Desktop/STAT27410/STAT27410-Final-Project/Dickey-Fuller.png")
```
Following the establishment of stationarity, we examined Autocorrelation Function (ACF) and Partial Autocorrelation Function (PACF) plots. These diagnostic tools provided crucial insights into the underlying structure of the time series, particularly in determining the presence and order of autoregressive (AR) and moving average (MA) components. The patterns observed in these plots served as preliminary indicators for model specification.
```{r, echo=FALSE, out.width = "80%", fig.align='center'}
knitr::include_graphics("/Users/scarlett_hjq/Desktop/STAT27410/STAT27410-Final-Project/acf_pacf.png")
```
The PACF suggests a potential AR component due to significant spikes at low lags.
Since there is small spikes with some lag on the ACF plot, it does not give strong indication of MA component selection. We may explore different MA components.

The lag of the spikes do not have a common divisor. Additionally, the period would be 252 trading days, and it is too large a number for the code to optimize the model. So we propose that there is no seasonality. 

To ensure optimal model selection, we fitted multiple ARIMA models to each financial time series, varying the orders of AR or MA each time, and compare them. Model performance was evaluated using established criteria such as AIC (Akaike Information Criterion) and BIC (Bayesian Information Criterion).

We compare between two selections:  one focusing on AR component, and another model focusing on MA component. 

First, We propose model ARIMA(6,0,1).
```{r, echo=FALSE, out.width = "80%", fig.align='center', warning=FALSE}
knitr::include_graphics("/Users/scarlett_hjq/Desktop/STAT27410/STAT27410-Final-Project/ts_601_summary.png")
```
The summary shows that the ar6 component have an coefficient absolute value greater than 0.05, meaning that this components perform some effect in the model.

Then, We propose model ARIMA(1,0,6).
```{r, echo=FALSE, out.width = "80%", fig.align='center', warning=FALSE}
knitr::include_graphics("/Users/scarlett_hjq/Desktop/STAT27410/STAT27410-Final-Project/ts_106_summary.png")
```
Similarly, the ma6 component have an coefficient absolute value greater than 0.05, meaning that this components perform some effect in the model.

We conducted model diagnostic by checking the residuals. We ensured model residuals fell within acceptable bounds and exhibited properties consistent with white noise processes. This validation process confirmed the adequacy of our selected models and their suitability for forecasting purposes.

```{r, echo=FALSE, out.width = "80%", fig.align='center'}
knitr::include_graphics("/Users/scarlett_hjq/Desktop/STAT27410/STAT27410-Final-Project/ts_601_diag.png")
knitr::include_graphics("/Users/scarlett_hjq/Desktop/STAT27410/STAT27410-Final-Project/ts_106_diag.png")
```
The diagnostics show that the residuals are not correlated with each other, meaning that both of our proposed model works.

When we compare the AIC and BIC, though the two values are similar for both graph, the model ARIMA(6,0,1) performs slightly better than ARIMA(1,0,6).
```{r, echo=FALSE, out.width = "80%", fig.align='center'}
knitr::include_graphics("/Users/scarlett_hjq/Desktop/STAT27410/STAT27410-Final-Project/AICBIC_compare.png")
```
Thus, we choose our proposed model ARIMA(6,0,1) as our model for prediction for CRM price.
The model parameters indicate significant autoregressive and moving average components, suggesting the stock price exhibits both momentum and mean-reverting characteristics.

### 3.1.2 Multiple Linear Regress (MLR) Models

Linear regression serves as a fundamental forecasting method. Multiple Linear Regression extends the basic model by incorporating various market factors, enabling deeper analysis of market behaviors.

Our methodology implements recursive prediction, with the initial model expressed as:
$$
\begin{aligned}
P_{i,t+1} &= \beta_1 P_{i,t} + \beta_2 P_{i,t-1} + \\
&\quad \beta_3 \text{QQQ}_{t} + \beta_4 \text{SPX}_{t}
\end{aligned}
$$
where $P_{i,t+1}$ represents the predicted price, $P_{i,t}$ and $P_{i,t-1}$ represent lagged prices, and $QQQ_t$, $SPX_t$ represent market indices at time t.

Model validation employs multiple diagnostic tests. We examine R-squared values and adjusted R-squared to assess model fit, F-statistics for overall significance, and t-statistics for individual variable significance. Residual analysis confirms assumptions of normality, homoscedasticity, and independence.

The recursive prediction process follows these steps:
1. Predict future price using lagged values
2. Update the dataset with predicted values
3. Adjust lagged variables for subsequent predictions
4. Iterate for next time period prediction
This recursive approach enables continuous updating of predictions while maintaining the temporal structure of the data.

## 3.2 Fitting the Frequentist Model(s)

## 3.2.1 Fitting Time Series Models

We discovered that time series data is better for fitting short term future data. The variance of the prediction increases as long as forecasting period extends, limiting the ability to provide insightful prediction. 

We discovered that the ARIMA model predicts a slight increase in mean stock price.
```{r, echo=FALSE, out.width = "60%", fig.align='center'}
knitr::include_graphics("/Users/scarlett_hjq/Desktop/STAT27410/STAT27410-Final-Project/CRM_ARIMA_forecast.png")
```



### 3.2.2 Fitting the MLR Models

Our Multiple Linear Regression model encounters same challenges in forecast accuracy. As predictions extend further from known data points, subsequent forecasts increasingly rely on previously predicted values rather than actual data. This dependency creates a compounding effect, leading to expanded variance and prediction intervals over time.

Analysis of the model coefficients, as shown in the regression summary, reveals that the most significant predictor variable is the immediate lagged price, with the highest t-value and lowest p-value. This finding aligns with market efficiency principles, suggesting that recent price information captures the most relevant market signals.

```{r, echo=FALSE, out.width = "90%", fig.align='center'}
knitr::include_graphics("/Users/scarlett_hjq/Desktop/STAT27410/STAT27410-Final-Project/CRM_MLR_summary.png")
```
However, the prediction is influenced by the fluctuations of stocks and is increased due to recursive prediction.
```{r, echo=FALSE, out.width = "90%", fig.align='center'}
knitr::include_graphics("/Users/scarlett_hjq/Desktop/STAT27410/STAT27410-Final-Project/CRM_MLR.png")
```
The visualization shows how the prediction intervals expand significantly as the forecast horizon extends, forming a characteristic "cone of uncertainty." This pattern is particularly evident in the latter portion of the forecast period, where the model's predictions exhibit increased oscillation and deviation from the central trend. This behavior underscores the limitations of recursive prediction in long-term forecasting applications, suggesting the model may be more reliable for shorter-term predictions where the cumulative impact of forecast errors remains contained.

## 3.3 Portfolio Weight Assignment
Our portfolio optimization methodology leverages the \textit{PerformanceAnalytics} R package to construct an efficient portfolio based on historical daily returns. While we currently demonstrate the approach using historical data, the methodology is designed to accommodate predicted future data for dynamic weight assignment across different time periods.

### Optimal Framework
The optimal portfolio weights, determined through our analysis, are distributed as follows:
```{r, echo=FALSE, out.width = "60%", fig.align='center'}
knitr::include_graphics("/Users/scarlett_hjq/Desktop/STAT27410/STAT27410-Final-Project/Portfolio_Weight_Assignment.png")
```
### Constraints and Objectives

We use mean-variance optimization framework which incorporates two fundamental constraints:

1. **Full Investment Constraint**:
   \[ \sum_{i=1}^n w_i = 1 \]
   where \(w_i\) represents the weight of asset i

2. **Long-Only Constraint**:
   \[ w_i \geq 0 \quad \forall i \]

The objective function maximizes the Sharpe Ratio while considering two key components:

\[ \text{Maximize} \quad \frac{E(R_p) - R_f}{\sigma_p} \]

where: 

* ***\(E(R_p)\)*** is the expected portfolio return
* ***\(R_f\)*** is the risk-free rate
* ***\(\sigma_p\)*** is the portfolio standard deviation

We can see the result from the risk-reward visualization chart. This single point in  the chart indicates that the optimization algorithm has identified one specific portfolio allocation that maximizes the Sharpe ratio  given our constraints. 
```{r, echo=FALSE, out.width = "60%", fig.align='center'}
knitr::include_graphics("/Users/scarlett_hjq/Desktop/STAT27410/STAT27410-Final-Project/risk_reward.png")
```
\newpage

# 4. Bayesian Analysis

## 4.1 Proposed Bayesian Model(s)

The Bayesian Model that will be explore in more depth is the Bayesian version of the frequentist ARMA model. Our model for the log return at time t is given by 

$$R_t = \phi(R_{t-1}) + \epsilon_t, \epsilon_t \sim N(0, \sigma)$$

where $R_t$ represents the log return at time t, $\phi$ represents the autocorrelation coefficient and $\sigma$ is the volatility of the stock.

We will further make the assumptions that the returns at any time $t$ has mean 0, and that the error term $\epsilon_t$ is not correlated with the returns at the previous time $t-1$. Additionally, we are conducting this analysis under the condition that the process is stationary, i.e $|\phi| < 1$. Given this, we can obtain the distribution for $R_t$ by computing the variance:

\begin{align*}
Var[R_t] &= Var[\phi(R_{t-1}) + \epsilon_t] \\ 
         &= \phi^2 Var[R_{t-1}] + Var[\epsilon_t] + 2\phi Cov(R_{t-1}, \epsilon_t) \\
         &= \phi^2 Var[R_t] + \sigma^2 + 0 \\
         &= \frac{\sigma^2}{1-\phi^2}\\
\end{align*}

Thus, we have that

$$R_1 \sim N\Bigl(0, \sqrt{\frac{\sigma^2}{1-\phi^2}}\Bigl) = N\Bigl(0, \frac{\sigma}{\sqrt{1-\phi^2}}\Bigl)$$

Then we have that for every future $R_t$, since we have $R_{t-1}$, it is distributed as

$$R_t \sim N\Bigl(\phi R_{t-1}, \sigma\Bigl)$$

For the priors on $\sigma$ and $\phi$, we will use the training data to compute approximate normal distributions for them. Then, to compute future returns, we will use the 20 previous closing prices prior to time $t$ as the likelihood, and compute the posterior distribution for $R_t$ for all $t$ in the test data set. 

## 4.2 Fitting the Bayesian model(s)

The models were fitted through Bayesian inference using the stan() function from the rstan package. 

Sensitivity analysis was done through looking at the trace plots at 50 day intervals of the test data. To ensure convergence, we used 5000 iterations and 4 chains for each fit. The trace plots are as follows:

\textbf{INSERT TRACE PLOTS}

Can see from all the trace plots that there seems to be very good mix between the chains, and suggest convergence.

To additionally check MCMC convergence, we used the monitor() function from the rstan package for each fit, and confirmed that all Rhat's satisfied Rhat < 1.05.

## 4.3 Prediction

To make prediction on the log return, we conducted a random draw from the posterior distribution, $R_t \sim N\Bigl(\phi R_{t-1}, \sigma\Bigl)$, as derived earlier. We then did this at every test data point, and converted the log returns into a predicted price. Below is a graph showing the two predictions


\textit{Have some questions about analyzing of predictions - MSE? R^2? maybe others?, would want something that penalizes larger differences more (hence why maybe MSE)}

# 5. Discussion

The biggest improvement that can be made to the model is the choice of what prior we are using. However, this is much easier said than done. There needs to be a balance between a very informative prior and a very non-informative prior. The very informative prior could lead to the prior dominating the likelihood, and mean that less weight is placed on our actual data. A very non-informative prior wouldn't be able to capture the underlying distribution of log returns, and focus too heavily on our actual data. Both approaches could struggle - the informative could struggle when there are sudden spikes or drops in stock price for whatever reason, and the non-informative could return unrealistic values (for instance a company perhaps going through unexpected explosive growth would be expected to continue that). As such, finding this actual balance is a hard task, and would largely differ across stocks and would be hard to generalize across stocks. 

Aside from prior selection, the length of time used for the likelihood data could also be optimized. 20 days is typically at least 4 weeks where a lot can happen. However, choosing too short of a time frame would cause the posterior to be too sensitive to changes in economic structure, while choosing too long of a time frame would be too uninformative.

As such, a big improvement that can be made to the model involves the selection of parameters, which likely differs across different individual stocks.

# 6. Contributions

In this section, discuss the percentage of your contributions to the development final project proposal. Report the number of hours you have worked on the proposal, and the sections you are involved.

Please also discuss briefly the contributions of your teammate(s), as well as the help and support you got from your teammates(s).

\newpage

# References
<div id="refs"></div>

\newpage

# Appendix

https://raw.githubusercontent.com/asu1-1/STAT27410-Final-Project/refs/heads/main/27410%20Final%20Project%20Code.R

https://raw.githubusercontent.com/asu1-1/STAT27410-Final-Project/refs/heads/main/Scarlett_part1_code.R

